{"cells":[{"cell_type":"markdown","source":["# Individual Feature Determination"],"metadata":{"id":"28sYRJfoFaXt"},"id":"28sYRJfoFaXt"},{"cell_type":"markdown","source":["## Preparation"],"metadata":{"id":"r8p1ntYzFhE0"},"id":"r8p1ntYzFhE0"},{"cell_type":"markdown","source":["### Import"],"metadata":{"id":"1xLE7wUVFjf_"},"id":"1xLE7wUVFjf_"},{"cell_type":"code","execution_count":null,"id":"78992abc","metadata":{"id":"78992abc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683629019789,"user_tz":-120,"elapsed":3045,"user":{"displayName":"Álvaro Francisco Gil","userId":"03286819362748434806"}},"outputId":"1f699c92-f402-4b75-af18-0a4a73c0996a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import pandas as pd\n","import os\n","import statistics\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["pd.set_option('display.max_rows', 100)"],"metadata":{"id":"fHFzQnE-vqod"},"id":"fHFzQnE-vqod","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Function Definition"],"metadata":{"id":"XQq_mqF7F6s1"},"id":"XQq_mqF7F6s1"},{"cell_type":"markdown","source":["### Prepare DataFrame"],"metadata":{"id":"8oQeW4lKSYkn"},"id":"8oQeW4lKSYkn"},{"cell_type":"code","source":["def preparation_filter_data(df):\n","\n","  # Change Id name\n","  df.rename(columns={'Speaker': 'ID'}, inplace=True)\n","\n","  # Calculate the duration for each speaker and add a new column to the DataFrame\n","  df[\"Duration\"] = df.apply(lambda row: int(row[\"End time (s)\"] - row[\"Start time (s)\"]), axis=1)\n","\n","  df = df[df[\"Duration\"] >= 1] \n","\n","  # Group lines by speaker and combine back-to-back lines\n","  df = df.groupby((df[\"ID\"] != df[\"ID\"].shift()).cumsum()).agg({\n","      \"ID\": \"first\",\n","      \"Start time\": \"first\",\n","      \"End time\": \"last\",\n","      \"Start time (s)\": \"first\",\n","      \"End time (s)\": \"last\",\n","      \"Subtitle\": lambda x: ' '.join(x),\n","      \"speech_neu\": \"mean\",\n","      \"speech_ang\": \"mean\",\n","      \"speech_hap\": \"mean\",\n","      \"speech_sad\": \"mean\",\n","      \"text_joy\": \"mean\",\n","      \"text_anger\":\"mean\",\n","      \"text_fear\": \"mean\",\n","      \"text_sadness\": \"mean\",\n","      \"Duration\": \"sum\"\n","  }).reset_index(drop=True)\n","\n","  return df"],"metadata":{"id":"7QiRz4qkSbAz"},"id":"7QiRz4qkSbAz","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"64f8b8dc","metadata":{"id":"64f8b8dc"},"source":["### Individual Time Spoken"]},{"cell_type":"code","execution_count":null,"id":"281e39b7","metadata":{"id":"281e39b7"},"outputs":[],"source":["def calc_individual_time_spoken(df,df_features):\n","\n","  # Calculate the duration for each speaker and add a new column to the DataFrame\n","  df[\"Duration\"] = df.apply(lambda row: int(row[\"End time (s)\"] - row[\"Start time (s)\"]), axis=1)\n","\n","  # Group the data by speaker and sum the durations\n","  duration_by_speaker = df.groupby(\"ID\")[\"Duration\"].sum().to_dict()\n","\n","  # Set the values in the indiv_spoken_time column based on speaker's ID\n","  for speaker_id, duration in duration_by_speaker.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'indiv_spoken_time'] = duration"]},{"cell_type":"markdown","source":["### Average Turn Duration"],"metadata":{"id":"RN82DFGlwRes"},"id":"RN82DFGlwRes"},{"cell_type":"code","source":["def avg_turn_duration(df,df_features):\n","\n","  # Calculate the duration for each speaker and add a new column to the DataFrame\n","  df[\"Duration\"] = df.apply(lambda row: int(row[\"End time (s)\"] - row[\"Start time (s)\"]), axis=1)\n","\n","  # Group the data by speaker and sum the durations\n","  duration_by_speaker = df.groupby(\"ID\")[\"Duration\"].mean().to_dict()\n","\n","  # Set the values in the indiv_spoken_time column based on speaker's ID\n","  for speaker_id, avg_duration in duration_by_speaker.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'average_turn_duration'] = avg_duration"],"metadata":{"id":"ERARGkFKwRyU"},"id":"ERARGkFKwRyU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Max and Avg Time without Speaking"],"metadata":{"id":"98QFQBwl1HE_"},"id":"98QFQBwl1HE_"},{"cell_type":"code","source":["def max_and_avg_time_without_speaking(df,df_features):\n","\n","    # Create dictionaries to store max and avg time\n","    unique_speakers = df['ID'].unique()\n","    max_time_speaker = {speaker: 0 for speaker in unique_speakers}\n","    times_speaker = {speaker: [] for speaker in unique_speakers}\n","\n","    # Iterate over each row in the DataFrame\n","    for index, row in df.iterrows():\n","        speaker = row['ID']\n","        end_time = row['End time (s)']\n","        n = 1\n","\n","        while index+n+1 < len(df) and df.loc[index + n, 'ID'] != speaker:\n","            n += 1\n","        \n","        if index + n < len(df):\n","            time_no_speak = df.loc[index + n, 'Start time (s)'] - end_time\n","            times_speaker[speaker].append(time_no_speak)\n","\n","            if time_no_speak > max_time_speaker[speaker]:\n","                max_time_speaker[speaker] = time_no_speak\n","\n","    times_speaker_avg = {key: statistics.mean(values) for key, values in times_speaker.items()}\n","    \n","    # Set the values in the indiv_spoken_time column based on speaker's ID\n","    for speaker_id, max_duration in max_time_speaker.items():\n","        df_features.loc[df_features['ID'] == speaker_id, 'max_time_without_speaking'] = max_duration\n","\n","    # Set the values in the indiv_spoken_time column based on speaker's ID\n","    for speaker_id, avg_duration in times_speaker_avg.items():\n","        df_features.loc[df_features['ID'] == speaker_id, 'avg_time_without_speaking'] = avg_duration\n"],"metadata":{"id":"AljiqcGo1RMf"},"id":"AljiqcGo1RMf","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"d441f3e4","metadata":{"id":"d441f3e4"},"source":["### Number of Turns"]},{"cell_type":"code","execution_count":null,"id":"f38ad0b9","metadata":{"id":"f38ad0b9"},"outputs":[],"source":["def calc_num_turns(df,df_features):\n","\n","  speaker_counts = df['ID'].value_counts().to_dict()\n","\n","  # Set the values in the indiv_spoken_time column based on speaker's ID\n","  for speaker_id, count in speaker_counts.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'num_turns'] = count"]},{"cell_type":"markdown","source":["### Max and Avg Turns without Speaking"],"metadata":{"id":"OhaP6RfSw1q9"},"id":"OhaP6RfSw1q9"},{"cell_type":"code","source":["def max_and_avg_turns_without_speaking(df,df_features):\n","\n","    # Create dictionaries to store max and avg time\n","    unique_speakers = df['ID'].unique()\n","    max_turns_speaker = {speaker: 0 for speaker in unique_speakers}\n","    turns_speaker = {speaker: [] for speaker in unique_speakers}\n","\n","    # Iterate over each row in the DataFrame\n","    for index, row in df.iterrows():\n","        speaker = row['ID']\n","        n = 1\n","\n","        while index+n+1 < len(df) and df.loc[index + n, 'ID'] != speaker:\n","            n += 1\n","        n=n-1\n","        if index + n < len(df):\n","            turns_speaker[speaker].append(n)\n","\n","            if n > max_turns_speaker[speaker]:\n","                max_turns_speaker[speaker] = n\n","\n","    turns_speaker_avg = {key: statistics.mean(values) for key, values in turns_speaker.items()}\n","    \n","    # Set the values in the indiv_spoken_time column based on speaker's ID\n","    for speaker_id, max_turns in max_turns_speaker.items():\n","        df_features.loc[df_features['ID'] == speaker_id, 'max_turns_without_speaking'] = max_turns\n","\n","    # Set the values in the indiv_spoken_time column based on speaker's ID\n","    for speaker_id, avg_turns in turns_speaker_avg.items():\n","        df_features.loc[df_features['ID'] == speaker_id, 'avg_turns_without_speaking'] = avg_turns\n"],"metadata":{"id":"a9xe383zw1q-"},"execution_count":null,"outputs":[],"id":"a9xe383zw1q-"},{"cell_type":"markdown","id":"cb8276d9","metadata":{"id":"cb8276d9"},"source":["### Number of Words"]},{"cell_type":"code","execution_count":null,"id":"b25bbf19","metadata":{"id":"b25bbf19"},"outputs":[],"source":["def calc_num_words(df,df_features):\n","\n","  # Split the subtitle column into words and count the number of words in each row\n","  df['word_count'] = df['Subtitle'].str.split().str.len()\n","\n","  # Group by speaker ID and sum the word counts\n","  word_counts = df.groupby('ID')['word_count'].sum().to_dict()\n","\n","  df = df.drop(columns=['word_count'])\n","\n","  for speaker_id, words in word_counts.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'num_words'] = words"]},{"cell_type":"markdown","source":["### Average Words per Turn"],"metadata":{"id":"W3Vzx1NT1125"},"id":"W3Vzx1NT1125"},{"cell_type":"code","source":["def avg_words_turn(df,df_features):\n","\n","  # Split the subtitle column into words and count the number of words in each row\n","  df['word_count'] = df['Subtitle'].str.split().str.len()\n","\n","  # Group by speaker ID and sum the word counts\n","  avg_word_counts = df.groupby('ID')['word_count'].mean().to_dict()\n","\n","  df = df.drop(columns=['word_count'])\n","\n","  for speaker_id, words in avg_word_counts.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'avg_words_turn'] = words"],"metadata":{"id":"nhWkonxI19z3"},"id":"nhWkonxI19z3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Max Words per Turn"],"metadata":{"id":"-X1We1Mt12I3"},"id":"-X1We1Mt12I3"},{"cell_type":"code","source":["def max_words_turn(df,df_features):\n","\n","  # Split the subtitle column into words and count the number of words in each row\n","  df['word_count'] = df['Subtitle'].str.split().str.len()\n","\n","  # Group by speaker ID and sum the word counts\n","  avg_word_counts = df.groupby('ID')['word_count'].max().to_dict()\n","\n","  df = df.drop(columns=['word_count'])\n","\n","  for speaker_id, words in avg_word_counts.items():\n","      df_features.loc[df_features['ID'] == speaker_id, 'max_words_turn'] = words"],"metadata":{"id":"fFIhedRS1-Eg"},"id":"fFIhedRS1-Eg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate Average of 1 Emotion"],"metadata":{"id":"CuTgJQWRYVE1"},"id":"CuTgJQWRYVE1"},{"cell_type":"code","source":["def calc_avg_one_emotion(df, df_features, col):\n","\n","    # Group by speaker ID and calculate the mean of the emotion column\n","    speakers_emotion = df.groupby('ID')[col].mean().to_dict()\n","\n","    for speaker_id, emotion in speakers_emotion.items():\n","        df_features.loc[df_features['ID'] == speaker_id, col] = emotion"],"metadata":{"id":"qZcWglyTUEFy"},"id":"qZcWglyTUEFy","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate Average of All Emotions"],"metadata":{"id":"2rYny1JlKGtm"},"id":"2rYny1JlKGtm"},{"cell_type":"code","source":["def calc_avg_emotions(df, df_features,start_col, end_col):\n","\n","    cols_range = df.columns[df.columns.get_loc(start_col):df.columns.get_loc(end_col)+1].tolist()\n","    \n","    # Loop through the emotion columns and calculate the average for each\n","    for col in cols_range:\n","        calc_avg_one_emotion(df, df_features, col)"],"metadata":{"id":"PGtI3UVyKGQu"},"id":"PGtI3UVyKGQu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Calculate All Individual Features for a Single Group"],"metadata":{"id":"YU8pm3GYJKo-"},"id":"YU8pm3GYJKo-"},{"cell_type":"code","source":["def calc_individual_features_dataframes(df_features,meeting_file):\n","\n","  # Read and prepare dataframe for analysis\n","  df= pd.read_csv(meeting_file)\n","  df = preparation_filter_data(df)\n","\n","  # Apply functions to calc features\n","  calc_individual_time_spoken(df,df_features)\n","  avg_turn_duration(df,df_features)\n","  max_and_avg_time_without_speaking(df,df_features)\n","  calc_num_turns(df,df_features)\n","  max_and_avg_turns_without_speaking(df,df_features)\n","  calc_num_words(df,df_features)\n","  avg_words_turn(df,df_features)\n","  max_words_turn(df,df_features)\n","  calc_avg_emotions(df, df_features,'speech_neu', 'text_sadness')\n","\n","  return df_features  "],"metadata":{"id":"4pUSzAHFG9Cc"},"id":"4pUSzAHFG9Cc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Determine Features for Each Group"],"metadata":{"id":"2eWS7V1FJULd"},"id":"2eWS7V1FJULd"},{"cell_type":"code","execution_count":null,"id":"a191e071","metadata":{"id":"a191e071"},"outputs":[],"source":["def determine_features_foreach_group(directory_in, indiv_features_blank_file, indiv_features_file_out):\n","\n","    df_features= pd.read_excel(indiv_features_blank_file)\n","  \n","    # Get a list of all CSV files in the directory\n","    csv_files = [f for f in os.listdir(directory_in) if os.path.isfile(os.path.join(directory_in, f)) and f.endswith('.csv')]\n","\n","    # Iterate over each CSV file and call calc_individual_features_dataframes\n","    for i, csv_file in enumerate(csv_files):\n","        meeting_file = os.path.join(directory_in, csv_file)\n","        df_features = calc_individual_features_dataframes(df_features, meeting_file)\n","        print(f\"Processed file {i+1}/{len(csv_files)}: {csv_file}\")\n","\n","    df_features.to_csv(indiv_features_file_out,encoding='utf-8-sig', index=False)\n"]},{"cell_type":"markdown","source":["## Use of Function"],"metadata":{"id":"UZhXb7j6GErP"},"id":"UZhXb7j6GErP"},{"cell_type":"code","source":["indiv_features_blank_file = r'/content/drive/MyDrive/Projects/tps/data/12. features/2_individual_features_byhand_values_filled.xlsx'\n","indiv_features_file_out = r'/content/drive/MyDrive/Projects/tps/data/12. features/3_individual_features_final.xlsx'\n","directory_in = r'/content/drive/MyDrive/Projects/tps/data/11. speech_text/old (may9)'"],"metadata":{"id":"IXwECopkgaiJ"},"id":"IXwECopkgaiJ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["determine_features_foreach_group(directory_in, indiv_features_blank_file, indiv_features_file_out)"],"metadata":{"id":"iE84F5hXIZdb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683629023115,"user_tz":-120,"elapsed":3354,"user":{"displayName":"Álvaro Francisco Gil","userId":"03286819362748434806"}},"outputId":"19be0ee9-b562-4340-a609-465dffeb1cb1"},"id":"iE84F5hXIZdb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processed file 1/12: 6_speech_text.csv\n","Processed file 2/12: 11_speech_text.csv\n","Processed file 3/12: 8_speech_text.csv\n","Processed file 4/12: 10_speech_text.csv\n","Processed file 5/12: 2_speech_text.csv\n","Processed file 6/12: 4_speech_text.csv\n","Processed file 7/12: 9_speech_text.csv\n","Processed file 8/12: 1_speech_text.csv\n","Processed file 9/12: 5_speech_text.csv\n","Processed file 10/12: 7_speech_text.csv\n","Processed file 11/12: 12_speech_text.csv\n","Processed file 12/12: 3_speech_text.csv\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[],"collapsed_sections":["d441f3e4","cb8276d9","CuTgJQWRYVE1","2rYny1JlKGtm","2eWS7V1FJULd"]}},"nbformat":4,"nbformat_minor":5}